## 单项选择题：
C、C、C、C、C、B、B、B、C

## 多项选择题：
A、B、C、D
A、B
A、B
B
A、B

## 附加题：

使用 ChatGPT 来给你解答本次课程中你还没有完全理解的概念，并将问答单独整理成文件或线上文档。示例如下：

问：为什么传统 RNN 的encoder decoder 架构会丢失信息？为什么要压缩到固定长度的向量

答：循环神经网络（RNN）的Encoder-Decoder架构的设计目标是处理序列到序列的问题，比如机器翻译、语音识别等。在这种架构中，Encoder的任务是接收一个可变长度的输入序列，并将其转换为一个固定长度的上下文向量。然后，Decoder使用这个上下文向量生成一个可变长度的输出序列。

这种架构的一个主要问题是，所有的输入序列信息都被编码到了一个固定长度的上下文向量中。当输入序列很长或者包含很多复杂信息时，一个固定长度的向量可能无法完全包含所有的信息，导致信息丢失。这就像试图将一本厚厚的书的全部内容压缩到一张纸条上，一些信息肯定会遗漏。

为什么要使用固定长度的向量呢？这主要是由于计算机处理数据的方式和神经网络的工作方式。计算机处理数据时，通常需要知道数据的大小和形状，以便于分配内存和进行有效的计算。同样，神经网络的结构（比如网络的层数、每层的节点数等）也是固定的，这要求输入数据的大小必须是确定的。因此，无论输入序列的长度如何，Encoder都需要生成一个固定长度的上下文向量。

为了解决这个问题，研究人员提出了一些方法，如长短期记忆（LSTM）和门控循环单元（GRU），它们可以更好地处理长序列。此外，注意力机制也被引入到了Encoder-Decoder架构中，它允许模型在生成每个输出词时，关注输入序列中的不同部分，而不是仅仅依赖于一个固定的上下文向量。

问：attetion 机制是如何解决信息丢失问题的

答：
与之前的模型不同，现在每一个输出词（Decoder中的红色部分）都有一个与之关联的Context Vector（绿色部分），而这个Context Vector是动态生成的，它对输入序列中的每个词的注意力分布进行了编码。

更具体地说，当Decoder在生成每一个输出词时，它都会通过Attention机制来决定应该将注意力集中在输入序列的哪些部分（Encoder中的蓝色部分）。这意味着在生成每一个输出词时，模型都会考虑输入序列中与当前输出词最相关的部分（找重点），而不是仅仅依赖于一个固定的Context Vector。

这就解决了原始Encoder-Decoder模型中的问题，因为现在模型可以更有效地利用输入序列的信息，特别是在处理长序列时，模型能够关注到与当前输出词最相关的输入词，从而减少了信息丢失的可能性。
